{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vectorizing_funcs import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering hard-coded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clustering_columns = [u'Asian', u'Black', u'Hispanic', u'Other', u'Unknown', u'White',\n",
    "       u'mouth_last', u'mouth_mean_slope',u'hands_last',\n",
    "       u'hands_mean_slope',u'onset_delta_last', u'ALSFRS_Total_last',\n",
    "       u'ALSFRS_Total_mean_slope',u'BMI_last', u'fvc_percent_mean_slope', \n",
    "                     u'respiratory_last', u'respiratory_mean_slope']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "We currently rank each feature family by regressing with it alone and comparing the regression score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import operator\n",
    "import time\n",
    "from sklearn.linear_model import LassoCV, LassoLarsCV\n",
    "\n",
    "def get_best_features_per_cluster(X, Y, all_feature_metadata):\n",
    "    best_features_per_cluster = {}\n",
    "    for c in X['cluster'].unique():\n",
    "        seg_X, seg_Y = X[X['cluster'] == c], Y[Y['cluster'] == c].ALSFRS_slope\n",
    "        seg_Y = seg_Y.fillna(seg_Y.mean())\n",
    "\n",
    "        score_per_feature = {}\n",
    "\n",
    "        for feature, fm in all_feature_metadata.iteritems():\n",
    "            regr = linear_model.LinearRegression()\n",
    "            X_feature_fam = seg_X[list(fm[\"derived_features\"])]\n",
    "            regr.fit(X_feature_fam, seg_Y)\n",
    "            score_per_feature[feature] = np.sqrt(np.mean((regr.predict(X_feature_fam) - seg_Y) ** 2))\n",
    "            regr.score(X_feature_fam, seg_Y)\n",
    "        best_features_per_cluster[c] = [k for k,v in sorted(score_per_feature.items(), key=operator.itemgetter(1))[:6]]\n",
    "    return best_features_per_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stepwise_best_features_per_cluster(X, Y, all_feature_metadata):\n",
    "    best_features_per_cluster = {}\n",
    "    for c in sorted(X['cluster'].unique()):\n",
    "        seg_X, seg_Y = X[X['cluster'] == c], Y[Y['cluster'] == c].ALSFRS_slope\n",
    "        print \"cluster:\", c, \"with size:\", seg_X.shape, \"with mean target:\", seg_Y.mean(), \"std:\", seg_Y.std()\n",
    "        seg_Y = seg_Y.fillna(seg_Y.mean())\n",
    "        \n",
    "        model = RandomForestRegressor(min_samples_leaf=60, random_state=0, n_estimators=1000)\n",
    "        #model = LassoCV(cv=5)\n",
    "        model = model.fit(seg_X, seg_Y)\n",
    "        \n",
    "        print \"best we can do with all features:\", np.sqrt(np.mean((model.predict(seg_X) - seg_Y) ** 2))\n",
    "        print \"using model:\", model\n",
    "\n",
    "        selected_fams = set()\n",
    "        selected_derived = set()\n",
    "        for i in range(6):\n",
    "            score_per_family = {}\n",
    "            t1 = time.time()\n",
    "            for family, fm in all_feature_metadata.iteritems():\n",
    "                if family not in selected_fams:                    \n",
    "                    X_feature_fam = seg_X[list(selected_derived) + list(fm[\"derived_features\"])]\n",
    "                    model = RandomForestRegressor(min_samples_leaf=60, random_state=0, n_estimators=1000)\n",
    "                    #model = LassoCV(cv=5)\n",
    "                    model = model.fit(X_feature_fam, seg_Y)\n",
    "                    score_per_family[family] = np.sqrt(np.mean((model.predict(X_feature_fam) - seg_Y) ** 2))\n",
    "            t_lasso_cv = time.time() - t1\n",
    "            best_fam = sorted(score_per_family.items(), key=operator.itemgetter(1))[0]\n",
    "            print \"adding best family:\", best_fam, \"time:\", t_lasso_cv\n",
    "            selected_fams.add(best_fam[0])\n",
    "            selected_derived.update(all_feature_metadata[best_fam[0]][\"derived_features\"])\n",
    "        best_features_per_cluster[c] = list(selected_fams)                          \n",
    "    return best_features_per_cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "def backward_best_features_per_cluster(X, Y, all_feature_metadata):\n",
    "    best_features_per_cluster = {}\n",
    "    for c in sorted(X['cluster'].unique()):\n",
    "        seg_X, seg_Y = X[X['cluster'] == c], Y[Y['cluster'] == c].ALSFRS_slope\n",
    "        print \"cluster:\", c, \"with size:\", seg_X.shape, \"with mean target:\", seg_Y.mean(), \"std:\", seg_Y.std()\n",
    "        seg_Y = seg_Y.fillna(seg_Y.mean())\n",
    "        \n",
    "        model = RandomForestRegressor(min_samples_leaf=60, random_state=0, n_estimators=1000).fit(seg_X, seg_Y)\n",
    "        print \"best we can do with all features:\", np.sqrt(np.mean((model.predict(seg_X) - seg_Y) ** 2))\n",
    "\n",
    "        selected_fams = set(all_feature_metadata.keys())\n",
    "        selected_derived = set([])\n",
    "        for fam in selected_fams:\n",
    "            selected_derived.update([der for der in all_feature_metadata[fam]['derived_features']])\n",
    "        while len(selected_fams) > 6:\n",
    "            score_per_family = {}\n",
    "            t1 = time.time()\n",
    "            for family, fm in all_feature_metadata.iteritems():\n",
    "                if family in selected_fams:\n",
    "                    X_feature_fam = seg_X[list(selected_derived - set(fm[\"derived_features\"]))]\n",
    "                    model = RandomForestRegressor(min_samples_leaf=60, random_state=0, n_estimators=1000).fit(\n",
    "                        X_feature_fam, seg_Y)\n",
    "                    score_per_family[family] = np.sqrt(np.mean((model.predict(X_feature_fam) - seg_Y) ** 2))\n",
    "            t_lasso_cv = time.time() - t1\n",
    "            worst_fam = sorted(score_per_family.items(), key=operator.itemgetter(1), reverse=True)[0]\n",
    "            print \"removing worst family:\", worst_fam, \"time:\", t_lasso_cv\n",
    "            selected_fams.remove(worst_fam[0])\n",
    "            selected_derived = set([])\n",
    "            for fam in selected_fams:\n",
    "                selected_derived.update([der for der in all_feature_metadata[fam]['derived_features']])\n",
    "        best_features_per_cluster[c] = list(selected_fams)                          \n",
    "    return best_features_per_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_only_selected_features(df, clusters, best_features_per_cluster, debug=False): \n",
    "    j = df.join(clusters)\n",
    "    buf, is_first = \"\", True\n",
    "    for c, features in best_features_per_cluster.iteritems():\n",
    "        slice = j[j.cluster == c]\n",
    "        selected = slice[slice.feature_name.isin(features)]\n",
    "        if debug:\n",
    "            print c, slice.shape, \" --> \", selected.shape\n",
    "        buf += selected.to_csv(sep='|', header = is_first, columns=df.columns)\n",
    "        is_first = False\n",
    "    return buf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prediction\n",
    "We use simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LassoCV, LassoLarsCV\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "def get_model_per_cluster(X, Y):\n",
    "    model_per_cluster = {}\n",
    "    for c in X.cluster.unique():    \n",
    "        X_cluster = X[X.cluster==c]\n",
    "        Y_true = Y[Y.cluster == c].ALSFRS_slope\n",
    "        \n",
    "        regr = LassoCV(cv=5)\n",
    "        regr.fit(X_cluster, Y_true)\n",
    "\n",
    "        print 'cluster: %d size: %s' % (c, Y_true.shape)\n",
    "        Y_predict = regr.predict(X_cluster)\n",
    "        print \"\\t RMS error (0 is perfect): %.2f\" % np.sqrt(np.mean(\n",
    "            (Y_predict - Y_true) ** 2))\n",
    "        regression_SS = ((Y_predict - Y_true) ** 2).sum()\n",
    "        residual_SS =((Y_true - Y_true.mean()) ** 2).sum()\n",
    "        print '\\t coefficient of determination R^2 = %.2f ' % (1.0 - regression_SS/residual_SS) # regr.score(X_cluster, Y_true)\n",
    "        cov = sum((Y_predict - Y_predict.mean())*(Y_true - Y_true.mean()))\n",
    "        Y_predict_std = np.sqrt(sum((Y_predict - Y_predict.mean())**2))\n",
    "        Y_true_std = np.sqrt(sum((Y_true - Y_true.mean())**2))\n",
    "        print '\\t pearson correlation r = %.2f ' % (cov/(Y_predict_std*Y_true_std)) # scipy.stats.pearsonr(Y_predict, Y_true)[0]\n",
    "        print \"3 sample predictions: \", regr.predict(X_cluster)[:3]\n",
    "        model_per_cluster[c] = {\"cluster_train_data_means\": X_cluster.mean(), \"model\" : regr}\n",
    "    return model_per_cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def apply_model(x, model_per_cluster):\n",
    "    c = x['cluster']\n",
    "    model = model_per_cluster[c]['model']\n",
    "    pred = float(model.predict(x))\n",
    "    return pd.Series({'prediction':pred, 'cluster': int(c)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation, grid_search\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from StringIO import StringIO\n",
    "\n",
    "def train_it(train_data, slope, my_n_clusters):\n",
    "        global ts_funcs_to_features\n",
    "        # Prepare metadata\n",
    "        ts_funcs_to_features = add_frequent_lab_tests_to_ts_features(train_data, ts_funcs_to_features)\n",
    "        all_feature_metadata = invert_func_to_features(ts_funcs_to_features, \"ts\")\n",
    "        all_feature_metadata.update(invert_func_to_features(dummy_funcs_to_features, \"dummy\"))\n",
    "        all_feature_metadata = learn_to_dummies_model(train_data, all_feature_metadata)\n",
    "        \n",
    "        # Vectorizing\n",
    "        vectorized, all_feature_metadata = vectorize(train_data, all_feature_metadata)\n",
    "        train_data_medians = vectorized.median()\n",
    "        train_data_mads = (vectorized - train_data_medians).abs().median()\n",
    "        train_data_std = vectorized.std()\n",
    "        cleaned = clean_outliers(vectorized, all_feature_metadata, train_data_medians, train_data_mads, train_data_std)\n",
    "        train_data_means = cleaned.mean()\n",
    "        train_data_std = cleaned.std()            \n",
    "        normalized, all_feature_metadata = normalize(cleaned, all_feature_metadata, train_data_means, train_data_std)\n",
    "\n",
    "        everybody = normalized.join(slope)\n",
    "        X = everybody.drop(['ALSFRS_slope'], 1)\n",
    "        Y = everybody[['ALSFRS_slope']]\n",
    "        print \"train_data: \", X.shape, Y.shape\n",
    "        \n",
    "        # Clustering\n",
    "        #for_clustering = normalized[clustering_columns]\n",
    "        #kmeans = KMeans(init='k-means++', n_clusters=my_n_clusters)\n",
    "        #clusters['cluster'] = kmeans.fit_predict(for_clustering)\n",
    "\n",
    "        forest = grid_search.GridSearchCV(RandomForestRegressor(min_samples_leaf=60, min_samples_split=260, random_state=0), \n",
    "                               {'min_samples_leaf': range(60,61,10), 'n_estimators': [1000]})\n",
    "        forest.fit(X, Y.ALSFRS_slope)\n",
    "        quart = 100 / my_n_clusters\n",
    "        bins = np.percentile(forest.predict(X), range(quart,100,quart))\n",
    "                          \n",
    "        # Note we must convert to str to join with slope later\n",
    "        clusters = pd.DataFrame(index = normalized.index.astype(str))\n",
    "        clusters['cluster'] = np.digitize(forest.predict(X), bins)\n",
    "        print \"train cluster cnt: \", np.bincount(clusters.cluster)\n",
    "\n",
    "        X = X.join(clusters)\n",
    "        Y = Y.join(clusters)\n",
    "\n",
    "        best_features_per_cluster = stepwise_best_features_per_cluster(X, Y, all_feature_metadata)\n",
    "        print \"best_features_per_cluster: \", best_features_per_cluster \n",
    "        buf = filter_only_selected_features(train_data.set_index(\"SubjectID\"), clusters, \\\n",
    "                                            best_features_per_cluster)\n",
    "\n",
    "        s_df = pd.read_csv(StringIO(buf), sep='|', index_col=False, dtype='unicode')\n",
    "        s_vectorized, _ = vectorize(s_df, all_feature_metadata)\n",
    "        # if we have a subject missing all selected features, fill him with missing values right before normalizing\n",
    "        s_vectorized = s_vectorized.join(Y, how = 'right')\n",
    "        s_vectorized = s_vectorized.drop('ALSFRS_slope', 1)\n",
    "\n",
    "        s_normalized, _ = normalize(s_vectorized, all_feature_metadata, train_data_means, train_data_std)    \n",
    "        s_X = s_normalized.join(clusters)\n",
    "        \n",
    "        model_per_cluster = get_model_per_cluster(s_X, Y)\n",
    "        \n",
    "        return all_feature_metadata, train_data_means, train_data_std, train_data_medians, train_data_mads,\\\n",
    "                     bins, forest, best_features_per_cluster, model_per_cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_on_test(test_data, all_feature_metadata, train_data_means, train_data_std, train_data_medians, train_data_mads,\n",
    "                 clustering_columns, bins, forest, best_features_per_cluster, model_per_cluster):\n",
    "    \n",
    "    # Vectorizing\n",
    "    vectorized, _ = vectorize(test_data, all_feature_metadata)\n",
    "    cleaned = clean_outliers(vectorized, all_feature_metadata, train_data_medians, train_data_mads, train_data_std)\n",
    "    normalized, _ = normalize(cleaned, all_feature_metadata, train_data_means, train_data_std)\n",
    "    \n",
    "    print \"applying on: \", normalized.shape\n",
    "    \n",
    "    # Clustering\n",
    "    \n",
    "    for_clustering = normalized\n",
    "    clusters = pd.DataFrame(index = for_clustering.index.astype(str))\n",
    "    clusters['cluster'] = np.digitize(forest.predict(for_clustering), bins)\n",
    "    print \"applied cluster cnt: \", np.bincount(clusters.cluster)\n",
    "\n",
    "    X = normalized.join(clusters)\n",
    "    \n",
    "    buf = filter_only_selected_features(test_data.set_index(\"SubjectID\"), clusters, \\\n",
    "                                        best_features_per_cluster)    \n",
    "    s_df = pd.read_csv(StringIO(buf), sep='|', index_col=False, dtype='unicode')\n",
    "    s_vectorized, _ = vectorize(s_df, all_feature_metadata)\n",
    "    s_cleaned = clean_outliers(s_vectorized, all_feature_metadata, train_data_medians, train_data_mads, train_data_std)\n",
    "    s_normalized, _ = normalize(s_cleaned, all_feature_metadata, train_data_means, train_data_std)    \n",
    "    input_for_model = s_normalized.join(clusters)    \n",
    "    \n",
    "    pred = input_for_model.apply(apply_model, args=[model_per_cluster], axis = 1)\n",
    "    return input_for_model, pred\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
